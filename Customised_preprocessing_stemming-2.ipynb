{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c8765b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets as skd\n",
    "categories = ['FS', 'Notes','Junk']\n",
    "pages_dict = skd.load_files('/Users/baggu/Downloads/FT_ML_training_Data/Input/', categories= categories, encoding= 'ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e85f2280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FS', 'Junk', 'Notes']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages_dict.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02dfbe0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "target_df = pd.DataFrame(pages_dict.target,columns =['Category'])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8,4))\n",
    "target_df.Category.value_counts().plot(kind='bar');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87bc0a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed only once\n",
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "import re, unidecode, string\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.stem.porter import PorterStemmer \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    stripped_text = soup.get_text(separator=\" \")\n",
    "    return stripped_text\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    text = unidecode.unidecode(text)\n",
    "    return text\n",
    "\n",
    "def remove_numbers(text): \n",
    "    result = re.sub(r'\\d+', '', text) \n",
    "    return result\n",
    "\n",
    "def remove_slash_with_space(text): \n",
    "    return text.replace('\\\\', \" \")\n",
    "\n",
    "def remove_punctuation(text): \n",
    "    translator = str.maketrans('', '', string.punctuation) \n",
    "    return text.translate(translator) \n",
    "\n",
    "def text_lowercase(text): \n",
    "    return text.lower()     \n",
    "\n",
    "def remove_whitespace(text): \n",
    "    return  \" \".join(text.split()) \n",
    "\n",
    "def remove_stopwords(text): \n",
    "    stop_words = set(stopwords.words(\"english\")) \n",
    "    word_tokens = word_tokenize(text) \n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words] \n",
    "    return ' '.join(filtered_text)\n",
    "\n",
    "def stem_words(text): \n",
    "    stemmer = PorterStemmer() \n",
    "    word_tokens = word_tokenize(text) \n",
    "    stems = [stemmer.stem(word) for word in word_tokens] \n",
    "    return ' '.join(stems)\n",
    "\n",
    "def lemmatize_words(text): \n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    word_tokens = word_tokenize(text) \n",
    "    # provide context i.e. part-of-speech \n",
    "    lemmas = [lemmatizer.lemmatize(word, pos ='v') for word in word_tokens] \n",
    "    return ' '.join(lemmas) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52856181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform preprocessing\n",
    "def perform_preprocessing(text):\n",
    "    text = remove_html_tags(text)\n",
    "    text = remove_accented_chars(text)\n",
    "    text = remove_numbers(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = text_lowercase(text)\n",
    "    text = remove_slash_with_space(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = stem_words(text)\n",
    "    #text = lemmatize_words(text)\n",
    "    text = remove_whitespace(text)\n",
    "    return text\n",
    "\n",
    "pages_dict.data = list(map(perform_preprocessing, pages_dict.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ea0e17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "data_train, data_test, target_train, target_test = train_test_split(pages_dict['data'], pages_dict['target'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5250af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pages_dict = {\n",
    "    'data' : data_train,\n",
    "    'target' : target_train\n",
    "}\n",
    "\n",
    "test_pages_dict = {\n",
    "    'data' : data_test,\n",
    "    'target' : target_test\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e02421b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer(stop_words='english', min_df=0.03, ngram_range=(1,2)) #I can also specify the range of n-grams\n",
    "X_train_tf = count_vect.fit_transform(train_pages_dict['data'])\n",
    "X_train_tf.shape\n",
    "\n",
    "# # Saving model to disk\n",
    "# import pickle\n",
    "# #import requests\n",
    "# #import json\n",
    "# pickle.dump(count_vect, open('CountVectorizer.pkl','wb'))\n",
    "# #0.03 is better than 0.02 & 0.04, 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bdb74dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assigning vectors to the input data\n",
    "# from sklearn.feature_extraction.text import TFidfVectorizer\n",
    "# tfidfconverter = TfidfVectorizer(stop_words='english', min_df=0.03, ngram_range=(1,2))\n",
    "# inputs = tfidfconverter.fit_transform(train_pages_dict['data'])\n",
    "# inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ae06ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# count_vect = CountVectorizer(stop_words='english', min_df=0.02, ngram_range=(1,2)) #I can also specify the range of n-grams\n",
    "# X_train_tf = count_vect.fit_transform(train_pages_dict['data'])\n",
    "# X_train_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f19cb06f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8372, 963)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_tf)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "045fccac",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "get_features not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/vl/wjwps_9d1yjd0fcnl88tddl40000gn/T/ipykernel_48751/3077211768.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    685\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetnnz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 687\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" not found\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: get_features not found"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bea782fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_tfidf, train_pages_dict['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b810988b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tf = count_vect.transform(test_pages_dict['data'])\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test_tf)\n",
    "predicted = clf.predict(X_test_tfidf) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95e28ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy(Naive Bayes): 0.8972766364070712\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          FS       0.91      0.92      0.91       197\n",
      "        Junk       0.95      0.88      0.92      1152\n",
      "       Notes       0.83      0.91      0.87       744\n",
      "\n",
      "    accuracy                           0.90      2093\n",
      "   macro avg       0.89      0.91      0.90      2093\n",
      "weighted avg       0.90      0.90      0.90      2093\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 182,    0,   15],\n",
       "       [   6, 1018,  128],\n",
       "       [  13,   53,  678]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy(Naive Bayes):',accuracy_score(test_pages_dict['target'],predicted))\n",
    "print(metrics.classification_report(test_pages_dict['target'],predicted,target_names=pages_dict.target_names))\n",
    "metrics.confusion_matrix(test_pages_dict['target'],predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b32a6583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model to disk\n",
    "import pickle\n",
    "#import requests\n",
    "#import json\n",
    "pickle.dump(clf, open('nbmodel.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51d9b91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy(RF): 0.9541328236980411\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          FS       0.99      0.95      0.97       197\n",
      "        Junk       0.96      0.96      0.96      1152\n",
      "       Notes       0.93      0.94      0.94       744\n",
      "\n",
      "    accuracy                           0.95      2093\n",
      "   macro avg       0.96      0.95      0.96      2093\n",
      "weighted avg       0.95      0.95      0.95      2093\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 188,    3,    6],\n",
       "       [   0, 1106,   46],\n",
       "       [   1,   40,  703]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import preprocessing\n",
    "classifier = RandomForestClassifier(n_estimators=1200, random_state=1)  #defining 1000 nodes \n",
    "rf = classifier.fit(X_train_tfidf, train_pages_dict['target'])  \n",
    "\n",
    "y_pred = classifier.predict(X_test_tfidf) \n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy(RF):',accuracy_score(test_pages_dict['target'],y_pred))\n",
    "print(metrics.classification_report(test_pages_dict['target'],y_pred,target_names=pages_dict.target_names))\n",
    "metrics.confusion_matrix(test_pages_dict['target'],y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2dcd2ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(rf, open('rfmodel.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b547ff8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy(RF): 0.9617773530817009\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          FS       0.99      0.96      0.97       197\n",
      "        Junk       0.96      0.97      0.97      1152\n",
      "       Notes       0.95      0.95      0.95       744\n",
      "\n",
      "    accuracy                           0.96      2093\n",
      "   macro avg       0.97      0.96      0.96      2093\n",
      "weighted avg       0.96      0.96      0.96      2093\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 189,    3,    5],\n",
       "       [   0, 1120,   32],\n",
       "       [   2,   38,  704]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install xgboost\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "xgb = XGBClassifier()\n",
    "xgb.fit(X_train_tfidf, train_pages_dict['target'])\n",
    "\n",
    "y_pred_xgb = xgb.predict(X_test_tfidf)\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy(RF):',accuracy_score(test_pages_dict['target'],y_pred_xgb))\n",
    "print(metrics.classification_report(test_pages_dict['target'],y_pred_xgb,target_names=pages_dict.target_names))\n",
    "metrics.confusion_matrix(test_pages_dict['target'],y_pred_xgb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "16b59868",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(xgb, open('xgbmodel.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "159ff65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Covert pdf to text file\n",
    "#!pip install pdfminer\n",
    "#!pip install io\n",
    "import io\n",
    "from io import StringIO\n",
    "import string\n",
    "import pandas as pd\n",
    "import os\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "\n",
    "def convert_pdf_to_txt(path):\n",
    "    #alltexts = []\n",
    "    filelist=os.listdir(path)\n",
    "    documentcollection=[]\n",
    "    for files in filelist:\n",
    "        files=os.path.join(path,files)\n",
    "        documentcollection.append(files)\n",
    "    for ifiles in documentcollection:\n",
    "        if ifiles.endswith('.pdf') or ifiles.endswith('.PDF'): #different extensions on the raw data\n",
    "            with open(ifiles, 'rb') as fh:\n",
    "                for page in PDFPage.get_pages(fh, \n",
    "                                              caching=True,\n",
    "                                              check_extractable=True):\n",
    "                    resource_manager = PDFResourceManager()\n",
    "                    fake_file_handle = io.StringIO()\n",
    "                    converter = TextConverter(resource_manager, fake_file_handle)\n",
    "                    page_interpreter = PDFPageInterpreter(resource_manager, converter)\n",
    "                    page_interpreter.process_page(page)\n",
    " \n",
    "                    text = fake_file_handle.getvalue() # extraction of the text data\n",
    "                    yield text\n",
    " \n",
    "                    # closing open handles\n",
    "                    converter.close()\n",
    "                    fake_file_handle.close()\n",
    "        \n",
    "    #return alltexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "befe8ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the saved model and a random PDF file to test classification\n",
    "savedmodel = pickle.load(open('xgbmodel.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e1f8391c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath='/Users/baggu/Downloads/PDF/'\n",
    "textcontents = convert_pdf_to_txt(filepath)\n",
    "dftaxes = pd.DataFrame(textcontents, columns = ['Text_Data'])\n",
    "##got the text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "20e1672c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      2021 2021 ANNUAL REPORT(A joint stock company ...\n",
       "1      ProfileThe predecessor of the Bank was Agricul...\n",
       "2      1Annual Report 2021Definitions2Basic Corporate...\n",
       "3      Definitions2In this report, unless the context...\n",
       "4      3Annual Report 2021Definitions13.H Share(s)Sha...\n",
       "                             ...                        \n",
       "359    358Notes to the Consolidated Financial Stateme...\n",
       "360    359Annual Report 2021Unaudited Supplementary F...\n",
       "361    360Unaudited Supplementary Financial Informati...\n",
       "362                                                    \n",
       "\n",
       "363    2021 2021 ANNUAL REPORT(A joint stock company ...\n",
       "Name: Text_Data, Length: 364, dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### preprocess the data, can also convert to list\n",
    "dftaxes['Text_Data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a7f3a3b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text_Data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021 2021 ANNUAL REPORT(A joint stock company ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ProfileThe predecessor of the Bank was Agricul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1Annual Report 2021Definitions2Basic Corporate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Definitions2In this report, unless the context...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3Annual Report 2021Definitions13.H Share(s)Sha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>358Notes to the Consolidated Financial Stateme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>359Annual Report 2021Unaudited Supplementary F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>360Unaudited Supplementary Financial Informati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>2021 2021 ANNUAL REPORT(A joint stock company ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>364 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Text_Data\n",
       "0    2021 2021 ANNUAL REPORT(A joint stock company ...\n",
       "1    ProfileThe predecessor of the Bank was Agricul...\n",
       "2    1Annual Report 2021Definitions2Basic Corporate...\n",
       "3    Definitions2In this report, unless the context...\n",
       "4    3Annual Report 2021Definitions13.H Share(s)Sha...\n",
       "..                                                 ...\n",
       "359  358Notes to the Consolidated Financial Stateme...\n",
       "360  359Annual Report 2021Unaudited Supplementary F...\n",
       "361  360Unaudited Supplementary Financial Informati...\n",
       "362                                                  \n",
       "\n",
       "363  2021 2021 ANNUAL REPORT(A joint stock company ...\n",
       "\n",
       "[364 rows x 1 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftaxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1422609a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## preprocess the data\n",
    "dftaxes.Text_Data = dftaxes.Text_Data.apply(perform_preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f2ff2b97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364, 1065)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Assigning vectors to the input data\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# count_vect = CountVectorizer(stop_words='english', min_df=0.03, ngram_range=(1,2)) #I can also specify the range of n-grams\n",
    "# inputs_ = count_vect.fit_transform(dftaxes.Text_Data)\n",
    "# #inputs_.shape\n",
    "# from sklearn.feature_extraction.text import TfidfTransformer\n",
    "# tfidf_transformer = TfidfTransformer()\n",
    "# inputs = tfidf_transformer.fit_transform(inputs_)\n",
    "# inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bcd661a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Feature shape mismatch, expected: 963, got 1065",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/vl/wjwps_9d1yjd0fcnl88tddl40000gn/T/ipykernel_48751/4263045786.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Predicting the category of the input file with the help of trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0moutput_category\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msavedmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m#Comment the next line if you are testing word2vec model as it doesn't require transformation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#output_category = (labelencoder.inverse_transform((output_category)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, output_margin, ntree_limit, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[1;32m   1432\u001b[0m         \u001b[0miteration_range\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1433\u001b[0m     ) -> np.ndarray:\n\u001b[0;32m-> 1434\u001b[0;31m         class_probs = super().predict(\n\u001b[0m\u001b[1;32m   1435\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1436\u001b[0m             \u001b[0moutput_margin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_margin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, output_margin, ntree_limit, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[1;32m   1047\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_use_inplace_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m                 predts = self.get_booster().inplace_predict(\n\u001b[0m\u001b[1;32m   1050\u001b[0m                     \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m                     \u001b[0miteration_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miteration_range\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36minplace_predict\u001b[0;34m(self, data, iteration_range, predict_type, missing, validate_features, base_margin, strict_shape)\u001b[0m\n\u001b[1;32m   2081\u001b[0m                 )\n\u001b[1;32m   2082\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2083\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m   2084\u001b[0m                     \u001b[0;34mf\"Feature shape mismatch, expected: {self.num_features()}, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2085\u001b[0m                     \u001b[0;34mf\"got {data.shape[1]}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Feature shape mismatch, expected: 963, got 1065"
     ]
    }
   ],
   "source": [
    "### count transform & tfidf transform\n",
    "### predict\n",
    "# Predicting the category of the input file with the help of trained model\n",
    "\n",
    "output_category = savedmodel.predict(inputs)\n",
    "#Comment the next line if you are testing word2vec model as it doesn't require transformation\n",
    "#output_category = (labelencoder.inverse_transform((output_category)))\n",
    "output_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ec84cf33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text_Data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>annual report a joint stock compani incorpor p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>profileth predecessor bank agricultur cooper b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>annual report definitionsbas corpor inform maj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>definitionsin report unless context otherwis r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>annual report definitionsh share share list th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>note consolid financi statementsfor year end d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>annual report unaudit supplementari financi in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>unaudit supplementari financi informationfor y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>annual report a joint stock compani incorpor p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>364 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Text_Data\n",
       "0    annual report a joint stock compani incorpor p...\n",
       "1    profileth predecessor bank agricultur cooper b...\n",
       "2    annual report definitionsbas corpor inform maj...\n",
       "3    definitionsin report unless context otherwis r...\n",
       "4    annual report definitionsh share share list th...\n",
       "..                                                 ...\n",
       "359  note consolid financi statementsfor year end d...\n",
       "360  annual report unaudit supplementari financi in...\n",
       "361  unaudit supplementari financi informationfor y...\n",
       "362                                                   \n",
       "363  annual report a joint stock compani incorpor p...\n",
       "\n",
       "[364 rows x 1 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftaxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808f16e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
