{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a16dba97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the text files as a key-value pair/dictionary\n",
    "\n",
    "import sklearn.datasets as skd\n",
    "categories = ['FS', 'Notes','Junk']\n",
    "\n",
    "#provide the directory to the Input folder. For example, my category sub-folders are present in Input folder.\n",
    "pages_dict = skd.load_files('/Users/baggu/Downloads/FT_ML_training_Data/Input/', categories= categories, encoding= 'ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34834e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the text data into test and train set\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "data_train, data_test, target_train, target_test = train_test_split(pages_dict['data'], pages_dict['target'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "494352a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating train and test dictionary for easy access\n",
    "\n",
    "train_pages_dict = {\n",
    "    'data' : data_train,\n",
    "    'target' : target_train\n",
    "}\n",
    "\n",
    "test_pages_dict = {\n",
    "    'data' : data_test,\n",
    "    'target' : target_test\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abdd74dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "#from nltk.stem import WordNetLemmatizer\n",
    "#lemmatizer = WordNetLemmatizer()\n",
    "from nltk.stem import PorterStemmer\n",
    "# init stemmer\n",
    "porter_stemmer=PorterStemmer()\n",
    "\n",
    "def my_cool_preprocessor(text):\n",
    "    text=text.lower()\n",
    "    text=re.sub(r'\\d+', '', text)\n",
    "    text=re.sub(\"\\\\W\",\" \",text) # remove special chars\n",
    "    #text=re.sub(\"\\\\s+(in|the|all|for|and|on)\\\\s+\",\" _connector_ \",text) # normalize certain words\n",
    "    \n",
    "    # stem words\n",
    "    words=re.split(\"\\\\s+\",text)\n",
    "    stemmed_words=[porter_stemmer.stem(word=word) for word in words]\n",
    "    #stemmed_words=[lemmatizer.lemmatize(word=word) for word in words]\n",
    "    return ' '.join(stemmed_words)\n",
    "#cv = CountVectorizer(cat_in_the_hat_docs,preprocessor=my_cool_preprocessor)\n",
    "#count_vector=cv.fit_transform(cat_in_the_hat_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7631aa11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/baggu/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8372, 978)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenizing the text document, eliminating english stop words, removing words with very less frequency\n",
    "#Creating count matrix\n",
    "#We also need to stem the stop_words\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer(stop_words='english',min_df=0.03, ngram_range=(1,2), preprocessor=my_cool_preprocessor) \n",
    "X_train_tf = count_vect.fit_transform(train_pages_dict['data'])\n",
    "X_train_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "040c9974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8372, 978)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating tf-idf matrix/ feature extraction using the count matrix\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_tf)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "198305e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy(Naive Bayes): 0.9001433349259437\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          FS       0.90      0.93      0.92       197\n",
      "        Junk       0.95      0.89      0.92      1152\n",
      "       Notes       0.83      0.91      0.87       744\n",
      "\n",
      "    accuracy                           0.90      2093\n",
      "   macro avg       0.89      0.91      0.90      2093\n",
      "weighted avg       0.90      0.90      0.90      2093\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 184,    0,   13],\n",
       "       [   8, 1021,  123],\n",
       "       [  13,   52,  679]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Classification model using Naive Bayes\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_tfidf, train_pages_dict['target']) #Naive Bayes model created\n",
    "\n",
    "X_test_tf = count_vect.transform(test_pages_dict['data'])\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test_tf)\n",
    "predicted = clf.predict(X_test_tfidf) #prediction of test data\n",
    "\n",
    "#Summary\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy(Naive Bayes):',accuracy_score(test_pages_dict['target'],predicted))\n",
    "print(metrics.classification_report(test_pages_dict['target'],predicted,target_names=pages_dict.target_names))\n",
    "metrics.confusion_matrix(test_pages_dict['target'],predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc701d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy(RF): 0.9550883898709985\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          FS       0.99      0.96      0.98       197\n",
      "        Junk       0.96      0.96      0.96      1152\n",
      "       Notes       0.93      0.94      0.94       744\n",
      "\n",
      "    accuracy                           0.96      2093\n",
      "   macro avg       0.96      0.96      0.96      2093\n",
      "weighted avg       0.96      0.96      0.96      2093\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 189,    3,    5],\n",
       "       [   0, 1107,   45],\n",
       "       [   1,   40,  703]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Classification model using random forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import preprocessing\n",
    "classifier = RandomForestClassifier(n_estimators=1200, random_state=1)  #defining 1000 nodes \n",
    "rf = classifier.fit(X_train_tfidf, train_pages_dict['target'])  \n",
    "\n",
    "y_pred = classifier.predict(X_test_tfidf) \n",
    "\n",
    "#Summary\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy(RF):',accuracy_score(test_pages_dict['target'],y_pred))\n",
    "print(metrics.classification_report(test_pages_dict['target'],y_pred,target_names=pages_dict.target_names))\n",
    "metrics.confusion_matrix(test_pages_dict['target'],y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bc2e9a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in ./opt/anaconda3/lib/python3.9/site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy in ./opt/anaconda3/lib/python3.9/site-packages (from xgboost) (1.20.3)\n",
      "Requirement already satisfied: scipy in ./opt/anaconda3/lib/python3.9/site-packages (from xgboost) (1.7.1)\n",
      "Accuracy(RF): 0.9627329192546584\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          FS       0.99      0.96      0.97       197\n",
      "        Junk       0.97      0.97      0.97      1152\n",
      "       Notes       0.95      0.95      0.95       744\n",
      "\n",
      "    accuracy                           0.96      2093\n",
      "   macro avg       0.97      0.96      0.96      2093\n",
      "weighted avg       0.96      0.96      0.96      2093\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 189,    3,    5],\n",
       "       [   0, 1118,   34],\n",
       "       [   2,   34,  708]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Classification model using xgb\n",
    "\n",
    "import sys\n",
    "!{sys.executable} -m pip install xgboost\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "xgb = XGBClassifier()\n",
    "xgb.fit(X_train_tfidf, train_pages_dict['target'])\n",
    "\n",
    "y_pred_xgb = xgb.predict(X_test_tfidf)\n",
    "\n",
    "#Summary\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy(RF):',accuracy_score(test_pages_dict['target'],y_pred_xgb))\n",
    "print(metrics.classification_report(test_pages_dict['target'],y_pred_xgb,target_names=pages_dict.target_names))\n",
    "metrics.confusion_matrix(test_pages_dict['target'],y_pred_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "808d9982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting auto-sklearn\n",
      "  Using cached auto_sklearn-0.14.7-py3-none-any.whl\n",
      "Collecting pynisher<0.7,>=0.6.3\n",
      "  Using cached pynisher-0.6.4-py3-none-any.whl\n",
      "Requirement already satisfied: numpy>=1.9.0 in ./opt/anaconda3/lib/python3.9/site-packages (from auto-sklearn) (1.20.3)\n",
      "Requirement already satisfied: typing-extensions in ./opt/anaconda3/lib/python3.9/site-packages (from auto-sklearn) (3.10.0.2)\n",
      "Requirement already satisfied: dask>=2021.12 in ./opt/anaconda3/lib/python3.9/site-packages (from auto-sklearn) (2022.5.0)\n",
      "Requirement already satisfied: distributed>=2012.12 in ./opt/anaconda3/lib/python3.9/site-packages (from auto-sklearn) (2021.10.0)\n",
      "Collecting distro\n",
      "  Using cached distro-1.7.0-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: scipy>=1.7.0 in ./opt/anaconda3/lib/python3.9/site-packages (from auto-sklearn) (1.7.1)\n",
      "Requirement already satisfied: pandas>=1.0 in ./opt/anaconda3/lib/python3.9/site-packages (from auto-sklearn) (1.3.4)\n",
      "Collecting liac-arff\n",
      "  Using cached liac_arff-2.5.0-py3-none-any.whl\n",
      "Collecting ConfigSpace<0.5,>=0.4.21\n",
      "  Using cached ConfigSpace-0.4.21-cp39-cp39-macosx_10_9_x86_64.whl (882 kB)\n",
      "Collecting smac<1.3,>=1.2\n",
      "  Using cached smac-1.2-py3-none-any.whl\n",
      "Collecting pyrfr<0.9,>=0.8.1\n",
      "  Using cached pyrfr-0.8.2.tar.gz (296 kB)\n",
      "Requirement already satisfied: pyyaml in ./opt/anaconda3/lib/python3.9/site-packages (from auto-sklearn) (6.0)\n",
      "Requirement already satisfied: threadpoolctl in ./opt/anaconda3/lib/python3.9/site-packages (from auto-sklearn) (2.2.0)\n",
      "Requirement already satisfied: scikit-learn<0.25.0,>=0.24.0 in ./opt/anaconda3/lib/python3.9/site-packages (from auto-sklearn) (0.24.2)\n",
      "Requirement already satisfied: joblib in ./opt/anaconda3/lib/python3.9/site-packages (from auto-sklearn) (1.1.0)\n",
      "Requirement already satisfied: setuptools in ./opt/anaconda3/lib/python3.9/site-packages (from auto-sklearn) (58.0.4)\n",
      "Requirement already satisfied: pyparsing in ./opt/anaconda3/lib/python3.9/site-packages (from ConfigSpace<0.5,>=0.4.21->auto-sklearn) (3.0.4)\n",
      "Requirement already satisfied: cython in ./opt/anaconda3/lib/python3.9/site-packages (from ConfigSpace<0.5,>=0.4.21->auto-sklearn) (0.29.24)\n",
      "Requirement already satisfied: cloudpickle>=1.1.1 in ./opt/anaconda3/lib/python3.9/site-packages (from dask>=2021.12->auto-sklearn) (2.0.0)\n",
      "Requirement already satisfied: partd>=0.3.10 in ./opt/anaconda3/lib/python3.9/site-packages (from dask>=2021.12->auto-sklearn) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in ./opt/anaconda3/lib/python3.9/site-packages (from dask>=2021.12->auto-sklearn) (2021.8.1)\n",
      "Requirement already satisfied: packaging>=20.0 in ./opt/anaconda3/lib/python3.9/site-packages (from dask>=2021.12->auto-sklearn) (21.0)\n",
      "Requirement already satisfied: toolz>=0.8.2 in ./opt/anaconda3/lib/python3.9/site-packages (from dask>=2021.12->auto-sklearn) (0.11.1)\n",
      "Requirement already satisfied: tblib>=1.6.0 in ./opt/anaconda3/lib/python3.9/site-packages (from distributed>=2012.12->auto-sklearn) (1.7.0)\n",
      "Collecting distributed>=2012.12\n",
      "  Using cached distributed-2022.5.0-py3-none-any.whl (856 kB)\n",
      "Requirement already satisfied: jinja2 in ./opt/anaconda3/lib/python3.9/site-packages (from distributed>=2012.12->auto-sklearn) (2.11.3)\n",
      "Requirement already satisfied: click>=6.6 in ./opt/anaconda3/lib/python3.9/site-packages (from distributed>=2012.12->auto-sklearn) (8.0.3)\n",
      "Requirement already satisfied: psutil>=5.0 in ./opt/anaconda3/lib/python3.9/site-packages (from distributed>=2012.12->auto-sklearn) (5.8.0)\n",
      "Requirement already satisfied: urllib3 in ./opt/anaconda3/lib/python3.9/site-packages (from distributed>=2012.12->auto-sklearn) (1.26.7)\n",
      "Requirement already satisfied: zict>=0.1.3 in ./opt/anaconda3/lib/python3.9/site-packages (from distributed>=2012.12->auto-sklearn) (2.0.0)\n",
      "Requirement already satisfied: tornado>=6.0.3 in ./opt/anaconda3/lib/python3.9/site-packages (from distributed>=2012.12->auto-sklearn) (6.1)\n",
      "Requirement already satisfied: locket>=1.0.0 in ./opt/anaconda3/lib/python3.9/site-packages (from distributed>=2012.12->auto-sklearn) (1.0.0)\n",
      "Requirement already satisfied: msgpack>=0.6.0 in ./opt/anaconda3/lib/python3.9/site-packages (from distributed>=2012.12->auto-sklearn) (1.0.2)\n",
      "Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in ./opt/anaconda3/lib/python3.9/site-packages (from distributed>=2012.12->auto-sklearn) (2.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in ./opt/anaconda3/lib/python3.9/site-packages (from pandas>=1.0->auto-sklearn) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in ./opt/anaconda3/lib/python3.9/site-packages (from pandas>=1.0->auto-sklearn) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in ./opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas>=1.0->auto-sklearn) (1.16.0)\n",
      "Collecting emcee>=3.0.0\n",
      "  Using cached emcee-3.1.2-py2.py3-none-any.whl (46 kB)\n",
      "Requirement already satisfied: heapdict in ./opt/anaconda3/lib/python3.9/site-packages (from zict>=0.1.3->distributed>=2012.12->auto-sklearn) (1.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in ./opt/anaconda3/lib/python3.9/site-packages (from jinja2->distributed>=2012.12->auto-sklearn) (1.1.1)\n",
      "Building wheels for collected packages: pyrfr\n",
      "  Building wheel for pyrfr (setup.py) ... \u001b[?25lerror\n",
      "\u001b[31m  ERROR: Command errored out with exit status 1:\n",
      "   command: /Users/baggu/opt/anaconda3/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/private/var/folders/vl/wjwps_9d1yjd0fcnl88tddl40000gn/T/pip-install-1a538p8j/pyrfr_8ac35394325947a5ac8cb0fb488547c1/setup.py'\"'\"'; __file__='\"'\"'/private/var/folders/vl/wjwps_9d1yjd0fcnl88tddl40000gn/T/pip-install-1a538p8j/pyrfr_8ac35394325947a5ac8cb0fb488547c1/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d /private/var/folders/vl/wjwps_9d1yjd0fcnl88tddl40000gn/T/pip-wheel-r9vzfw6w\n",
      "       cwd: /private/var/folders/vl/wjwps_9d1yjd0fcnl88tddl40000gn/T/pip-install-1a538p8j/pyrfr_8ac35394325947a5ac8cb0fb488547c1/\n",
      "  Complete output (7 lines):\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_ext\n",
      "  building 'pyrfr._regression' extension\n",
      "  swigging pyrfr/regression.i to pyrfr/regression_wrap.cpp\n",
      "  swig -python -c++ -modern -py3 -features nondynamic -I./include -o pyrfr/regression_wrap.cpp pyrfr/regression.i\n",
      "  error: command 'swig' failed: No such file or directory\n",
      "  ----------------------------------------\u001b[0m\n",
      "\u001b[31m  ERROR: Failed building wheel for pyrfr\u001b[0m\n",
      "\u001b[?25h  Running setup.py clean for pyrfr\n",
      "Failed to build pyrfr\n",
      "Installing collected packages: pyrfr, pynisher, emcee, distributed, ConfigSpace, smac, liac-arff, distro, auto-sklearn\n",
      "    Running setup.py install for pyrfr ... \u001b[?25lerror\n",
      "\u001b[31m    ERROR: Command errored out with exit status 1:\n",
      "     command: /Users/baggu/opt/anaconda3/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/private/var/folders/vl/wjwps_9d1yjd0fcnl88tddl40000gn/T/pip-install-1a538p8j/pyrfr_8ac35394325947a5ac8cb0fb488547c1/setup.py'\"'\"'; __file__='\"'\"'/private/var/folders/vl/wjwps_9d1yjd0fcnl88tddl40000gn/T/pip-install-1a538p8j/pyrfr_8ac35394325947a5ac8cb0fb488547c1/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /private/var/folders/vl/wjwps_9d1yjd0fcnl88tddl40000gn/T/pip-record-6wzugvfp/install-record.txt --single-version-externally-managed --compile --install-headers /Users/baggu/opt/anaconda3/include/python3.9/pyrfr\n",
      "         cwd: /private/var/folders/vl/wjwps_9d1yjd0fcnl88tddl40000gn/T/pip-install-1a538p8j/pyrfr_8ac35394325947a5ac8cb0fb488547c1/\n",
      "    Complete output (6 lines):\n",
      "    running install\n",
      "    running build_ext\n",
      "    building 'pyrfr._regression' extension\n",
      "    swigging pyrfr/regression.i to pyrfr/regression_wrap.cpp\n",
      "    swig -python -c++ -modern -py3 -features nondynamic -I./include -o pyrfr/regression_wrap.cpp pyrfr/regression.i\n",
      "    error: command 'swig' failed: No such file or directory\n",
      "    ----------------------------------------\u001b[0m\n",
      "\u001b[31mERROR: Command errored out with exit status 1: /Users/baggu/opt/anaconda3/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/private/var/folders/vl/wjwps_9d1yjd0fcnl88tddl40000gn/T/pip-install-1a538p8j/pyrfr_8ac35394325947a5ac8cb0fb488547c1/setup.py'\"'\"'; __file__='\"'\"'/private/var/folders/vl/wjwps_9d1yjd0fcnl88tddl40000gn/T/pip-install-1a538p8j/pyrfr_8ac35394325947a5ac8cb0fb488547c1/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /private/var/folders/vl/wjwps_9d1yjd0fcnl88tddl40000gn/T/pip-record-6wzugvfp/install-record.txt --single-version-externally-managed --compile --install-headers /Users/baggu/opt/anaconda3/include/python3.9/pyrfr Check the logs for full command output.\u001b[0m\n",
      "\u001b[?25hNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install auto-sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71920ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from autosklearn.classification import AutoSklearnClassifier\n",
    "# model = AutoSklearnClassifier(time_left_for_this_task=600,\n",
    "#                             max_models_on_disc=5,\n",
    "#                             metric = average_precision,\n",
    "#                             scoring_functions=[roc_auc, average_precision, accuracy, f1, precision, recall, log_loss])\n",
    "# # perform the search\n",
    "# model.fit(X_train_tfidf, train_pages_dict['target'])\n",
    "# model.sprint_statistics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
